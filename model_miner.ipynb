{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signal_miner import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from numerapi import NumerAPI\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and initialize data\n",
    "napi = NumerAPI()\n",
    "napi.download_dataset(\"v5.0/train.parquet\", \"v5.0/train.parquet\")\n",
    "napi.download_dataset(\"v5.0/validation.parquet\", \"v5.0/validation.parquet\")\n",
    "\n",
    "data = pd.concat([ pd.read_parquet(\"v5.0/train.parquet\"), pd.read_parquet(\"v5.0/validation.parquet\") ])\n",
    "\n",
    "targets = [t for t in data.columns if 'target' in t]\n",
    "feature_cols = [c for c in data.columns if 'feature' in c]\n",
    "\n",
    "data['era'] = data['era'].astype('int')\n",
    "data[targets] = (data[targets] * 4).astype('Int8')\n",
    "\n",
    "eras = np.array(sorted(data['era'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the benchmark configuration\n",
    "benchmark_cfg = {\n",
    "    \"colsample_bytree\": 0.1,\n",
    "    \"max_bin\": 5,\n",
    "    \"max_depth\": 5,\n",
    "    \"num_leaves\": 2**4-1,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"n_estimators\": 2000,\n",
    "    \"reg_lambda\": 0.0,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"target\": 'target'  # Use the first target for simplicity\n",
    "}\n",
    "\n",
    "# Parameter dictionary\n",
    "param_dict = {\n",
    "    'colsample_bytree': list(np.linspace(0.001, 1, 100)), \n",
    "    'reg_lambda': list(np.linspace(0, 100_000, 10000)),\n",
    "    'learning_rate': list( np.linspace(.00001, 1.0, 1000, dtype='float') ),\n",
    "    'max_bin' : list(np.linspace(2, 5, 4, dtype='int')),\n",
    "    'max_depth': list(np.linspace(2, 12, 11, dtype='int')),# [5, 10, 15, 20, 25, 50, 100],\n",
    "    'num_leaves': list(np.linspace(2, 24, 15, dtype='int')),#, 4112],#, 8192, 32768],\n",
    "    'min_child_samples': list( np.linspace(1,250,250,dtype='int') ),\n",
    "    'n_estimators': list( np.linspace(10,2000,1990,dtype='int') ),#,75,100,150,200],#, 500, 700, 900, 1200], \n",
    "    'target':targets,\n",
    "}\n",
    "\n",
    "# Cross-validation splits\n",
    "ns = 2\n",
    "all_splits = list(TimeSeriesSplit(n_splits=ns, max_train_size=100_000_000, gap=12).split(eras))\n",
    "\n",
    "niter = 1000  # Small number for testing\n",
    "configurations = get_rdn_cfgs(param_dict, niter)\n",
    "\n",
    "# Add benchmark configuration as the first entry\n",
    "configurations.insert(0, benchmark_cfg)\n",
    "\n",
    "# Keep track of the benchmark ID (always 0 in this case)\n",
    "BENCHMARK_ID = 0\n",
    "\n",
    "# Prepare memory-mapped files\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "mmapped_array = np.memmap(\n",
    "    os.path.join(\"results\", \"test_mmapped_array.dat\"),\n",
    "    dtype='float16', mode='w+', shape=(len(data), len(configurations))\n",
    ")\n",
    "done_splits = np.memmap(\n",
    "    os.path.join(\"results\", \"test_done_splits.dat\"),\n",
    "    dtype='float16', mode='w+', shape=(len(all_splits) * len(configurations))\n",
    ")\n",
    "data['mmap_idx'] = range(len(data))\n",
    "\n",
    "# Define the processing function\n",
    "def process_split(task):\n",
    "    train_didxs, test_didxs, k, cfg, split_id, split_num = task\n",
    "    try:\n",
    "        label = cfg['target']\n",
    "        train_rows = (data['era'].isin(eras[train_didxs])) & (~data[label].isna())\n",
    "        test_rows = (data['era'].isin(eras[test_didxs])) & (~data[label].isna())\n",
    "\n",
    "        model = get_model(cfg)\n",
    "\n",
    "        model.fit(\n",
    "            data.loc[train_rows, feature_cols].values,\n",
    "            data.loc[train_rows, label].values\n",
    "        )\n",
    "\n",
    "        result_vector = model.predict(data.loc[test_rows, feature_cols].values)\n",
    "        mmapped_array[data.loc[test_rows, 'mmap_idx'].values, k] = result_vector\n",
    "        done_splits[split_id] = 1\n",
    "        done_splits.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_split: {e}\")\n",
    "\n",
    "# Start mining\n",
    "pool = Pool(processes=2)\n",
    "\n",
    "def start_mining():\n",
    "    split_data_list = []\n",
    "    split_id = 0\n",
    "    for k, cfg in enumerate(configurations):\n",
    "        for i, (train_didxs, test_didxs) in enumerate(all_splits):\n",
    "            task = (train_didxs, test_didxs, k, cfg, split_id, i)\n",
    "            split_data_list.append(task)\n",
    "            split_id += 1\n",
    "\n",
    "    pool.imap(process_split, split_data_list, chunksize=1)\n",
    "\n",
    "# Check progress\n",
    "def check_progress():\n",
    "    completed = np.sum(done_splits)\n",
    "    total = len(done_splits)\n",
    "    print(f\"Progress: {completed}/{total} ({(completed / total) * 100:.2f}%)\")\n",
    "\n",
    "# End mining\n",
    "def end_mining():\n",
    "    global pool\n",
    "    if pool is not None:\n",
    "        pool.terminate()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pool = None\n",
    "\n",
    "# Example usage\n",
    "start_mining()  # Start mining asynchronously\n",
    "# check_progress()  # Check progress\n",
    "# end_mining()  # Stop the pool when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Check Progress Periodically'''\n",
    "check_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get Dataframe of Results\n",
    "'''\n",
    "res_df = evaluate_completed_configs(data, configurations, mmapped_array, done_splits, all_splits, ns)\n",
    "# Add a column to label benchmark configurations\n",
    "res_df['is_benchmark'] = res_df.index == BENCHMARK_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark Results:\")\n",
    "res_df[res_df['is_benchmark']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Show All Models with Corr and Sharpe higher than benchmark on Entire Data Set\n",
    "\n",
    "'''\n",
    "print(\"Better Than Benchmark Results:\")\n",
    "compare_to_benchmark(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize performance as a function of hyper params\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''''show some eval metrics based on each hyperparam, also only see this if we want, should be packages in its own method'''\n",
    "for col in res_df.select_dtypes(include= np.number).columns:\n",
    "    plt.scatter(res_df[col], res_df['eval_shp'], label='test')\n",
    "    plt.scatter(res_df[col], res_df['train_shp'], label='train')\n",
    "    plt.xlabel(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTANT\n",
    "Understand This Plot\n",
    "Shows the relationship of past fold to future fold performance\n",
    "'''\n",
    "# Identify the benchmark configuration\n",
    "benchmark_idx = BENCHMARK_ID\n",
    "benchmark_validation_shp = res_df.loc[benchmark_idx, 'validation_shp']\n",
    "benchmark_eval_shp = res_df.loc[benchmark_idx, 'test_shp']\n",
    "\n",
    "# Scatter plot with enhancements\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(res_df['validation_shp'], res_df['test_shp'], alpha=0.7, label=\"Random Configurations\")\n",
    "\n",
    "# Highlight the benchmark model as a red star\n",
    "plt.scatter(benchmark_validation_shp, benchmark_eval_shp, color='red', s=100, marker='*', label=\"Benchmark Model\")\n",
    "\n",
    "# Add a linear best-fit line\n",
    "x = res_df['validation_shp']\n",
    "y = res_df['test_shp']\n",
    "m, b = np.polyfit(x, y, 1)  # Fit line: y = mx + b\n",
    "plt.plot(x, m*x + b, color='blue', linestyle='--', label=f\"Best Fit Line (y={m:.2f}x + {b:.2f})\")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"Validation Sharpe\")\n",
    "plt.ylabel(\"Test Sharpe\")\n",
    "plt.title(\"Relationship Between Past Fold and Future Fold Performance\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_export = [ res_df.sort_values('whole_shp').iloc[-1].name ] #can be a list to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_ensemble(to_export, configurations, mmapped_array, data, all_splits, feature_cols, get_model, save_name=\"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
